# -*- coding: utf-8 -*-
"""Intern project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/lanamedea/intern-project.8bd8e611-9564-4bf9-a81e-1559b6d9e756.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250715/auto/storage/goog4_request%26X-Goog-Date%3D20250715T164710Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db54aa801e769794083540c801080c3f7f5b7cb6fc8fb91ed20a67fa8351219462bb26cd60110f3496c6c9d298d2f5b01a4b12e2f0798a38ab92a5054c2b0a318009bad6f84fb229b2a3c21503bc7d5f983ab65cdd7993fc2a9d3c53676860e2643028f841d42cabc61e2bdb7422213822f8a0ceed542e83b2558ae0e7e01d07a95187f608fc9e1c4876ca994574a776e8e212e2c70cdab6822489d15277d24fb83b5a2b44f341fe8f52d910f57ebc2f299a04d45060785e204b3f08315e93a4ba9d95af94f6b131670c24cdff68ea8c8b0f83af80c31b26b7dded0681bc8dd494a5396599fd21053c321314ad51c57708beae468d06e907df87ae84dcf16a786
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
vijayaadithyanvg_car_price_predictionused_cars_path = kagglehub.dataset_download('vijayaadithyanvg/car-price-predictionused-cars')

print('Data source import complete.')

"""**Data science intern project**

In this project, we aim to build a robust machine learning model to predict the selling price of used cars based on various features such as present price, kilometers driven, fuel type, transmission, and ownership history. The workflow begins with importing and exploring the dataset to understand its structure, identify missing values, and assess data types. Following this, we perform essential data cleaning, which includes engineering new features like the age of the car, dropping irrelevant columns , and encoding categorical variables using one-hot encoding to convert them into a format suitable for machine learning models. Once the data is preprocessed, we define the target variable Selling_Price and split the dataset into training and testing subsets. We then train an XGBoost regression model — a powerful and scalable tree-based algorithm known for its accuracy in tabular data — using the training set. After training, the model is evaluated on the test set using performance metrics like R² Score and Root Mean Squared Error (RMSE) to measure prediction accuracy. Upon achieving satisfactory results, we save the trained model using the joblib library, making it reusable for deployment. This saved model will later be integrated into an interactive Streamlit dashboard, allowing users to input vehicle details and receive real-time price predictions, thereby simulating a real-world data product

1- **Data Loading and Initial Exploration**

The first step involves importing the dataset into a Pandas DataFrame and conducting an initial exploratory analysis to understand its structure. This includes reviewing the number of rows and columns, checking for missing values, identifying data types, and generating descriptive statistics for numerical features. The purpose of this step is to develop a foundational understanding of the dataset and identify any immediate issues that may need to be addressed during preprocessing
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/kaggle/input/car-price-predictionused-cars/car data.csv")
df.head()

df.shape

df.isnull().sum()

df.describe()

df.columns

print(df.columns)

df = pd.get_dummies(df, drop_first=True)

"""2-**Data Cleaning and Feature Engineering**

In this phase, we clean and prepare the data for modeling. This includes removing any irrelevant or redundant columns.
"""

df.head()
df.columns
df.shape

df.duplicated().sum()

df = df.drop_duplicates()

print("The duplicate values is:", df.duplicated().sum())

df.describe().style.format(precision=2).background_gradient(cmap='viridis')

df = pd.read_csv("/kaggle/input/car-price-predictionused-cars/car data.csv")
df['Car_Age'] = 2025 - df['Year']
df = pd.get_dummies(df.drop(['Year', 'Car_Name'], axis=1), drop_first=True)
df = df.loc[:, ~df.columns.str.startswith('Car_Name_')]

import pandas as pd

df = pd.read_csv('/kaggle/input/car-price-predictionused-cars/car data.csv')

mean_values = df[['Selling_Price', 'Present_Price', 'Driven_kms', 'Owner']].mean()
std_values = df[['Selling_Price', 'Present_Price', 'Driven_kms', 'Owner']].std()

print("Means:")
print(mean_values)
print("\nStandard Deviations:")
print(std_values)

import pandas as pd

df = pd.read_csv('/kaggle/input/car-price-predictionused-cars/car data.csv')

summary = df.groupby('Car_Name').agg({
    'Year': ['min', 'max'],
    'Selling_Price': ['mean', 'std', 'min', 'max'],
    'Present_Price': ['mean', 'std'],
    'Driven_kms': ['mean', 'std'],
    'Owner': ['mean']
}).reset_index()

summary.columns = ['Car_Name', 'Year_Min', 'Year_Max',
                   'Selling_Price_Mean', 'Selling_Price_Std', 'Selling_Price_Min', 'Selling_Price_Max',
                   'Present_Price_Mean', 'Present_Price_Std',
                   'Kms_Driven_Mean', 'Kms_Driven_Std',
                   'Owner_Mean']

print(summary.head(10))

"""3- **Data visualization**  """

df['Car_Age'] = 2025 - df['Year']
import plotly.express as px

fig = px.scatter(df,
                 x='Present_Price',
                 y='Selling_Price',
                 color='Fuel_Type',
                 size='Car_Age',
                 hover_data=['Driven_kms', 'Owner', 'Transmission'])
fig.update_layout(title='Car Comparison: Present vs Selling Price',
                  xaxis_title='Present Price ',
                  yaxis_title='Selling Price ')
fig.show()

top_cars = df[['Car_Name', 'Present_Price', 'Selling_Price', 'Driven_kms', 'Owner', 'Fuel_Type', 'Transmission']]
top_cars = top_cars.sort_values(by='Selling_Price', ascending=False).head(10)
top_cars.style.set_caption("Top 10 Highest Selling Cars") \
       .background_gradient(cmap="Blues") \
       .format({'Present_Price': '₹ {:.2f} L', 'Selling_Price': '₹ {:.2f} L'})

df['Depreciation'] = df['Present_Price'] - df['Selling_Price']
df['Depreciation_%'] = (df['Depreciation'] / df['Present_Price']) * 100

compare_df = df[['Car_Name', 'Present_Price', 'Selling_Price', 'Depreciation', 'Depreciation_%']]
compare_df = compare_df.sort_values(by='Depreciation_%', ascending=False).head(10)
compare_df.style.set_caption("Top 10 Cars with Highest Depreciation") \
         .bar(subset='Depreciation_%', color='#ffa07a') \
         .format({'Present_Price': '₹ {:.2f} L', 'Selling_Price': '₹ {:.2f} L', 'Depreciation_%': '{:.1f}%'})

df['Car_Age'] = 2025 - df['Year']
summary = {
    'Total Cars': len(df),
    'Avg Present Price': df['Present_Price'].mean(),
    'Avg Selling Price': df['Selling_Price'].mean(),
    'Avg Car Age': df['Car_Age'].mean(),
    'Fuel Type Counts': df['Fuel_Type'].value_counts().to_dict()
}
pd.DataFrame.from_dict(summary, orient='index', columns=['Value']).style.set_caption("Market Summary ?")

fig2 = px.pie(df,
              names='Fuel_Type',
              title=" Fuel Type Distribution in Market",
              hole=0.4,
              color_discrete_sequence=px.colors.sequential.RdBu)

fig2.show()

fig3 = px.box(df,
              x='Transmission',
              y='Selling_Price',
              title="Selling Price by Transmission Type",
              color='Transmission')

fig3.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/kaggle/input/car-price-predictionused-cars/car data.csv')

categorical_columns = ['Fuel_Type', 'Selling_type', 'Transmission']

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))
axes = axes.ravel()

for i, column in enumerate(categorical_columns):
    sns.countplot(x=column, data=df, palette='Set2', ax=axes[i], saturation=0.9)
    for container in axes[i].containers:
        axes[i].bar_label(container, color='black', size=10)
    axes[i].set_title(f'{column.replace("_", " ")}', fontsize=12)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('Count')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

cat_cols = ['Fuel_Type', 'Selling_type', 'Transmission', 'Owner']

for i in range(0, len(cat_cols), 2):
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    sns.countplot(x=cat_cols[i], data=df, palette='pastel', ax=axes[0])
    axes[0].set_title(cat_cols[i])
    axes[0].set_xlabel('')
    axes[0].set_ylabel('Count')
    for container in axes[0].containers:
        axes[0].bar_label(container, color='black', fontsize=10)

    if i+1 < len(cat_cols):
        sns.countplot(x=cat_cols[i+1], data=df, palette='pastel', ax=axes[1])
        axes[1].set_title(cat_cols[i+1])
        axes[1].set_xlabel('')
        axes[1].set_ylabel('Count')
        for container in axes[1].containers:
            axes[1].bar_label(container, color='black', fontsize=10)
    else:
        axes[1].axis('off')

    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.barplot(x='Year', y='Selling_Price', data=df, errorbar=None, palette='pastel')
plt.title('Average Selling Price by Year', fontsize=16)
plt.xlabel('Year', fontsize=12)
plt.ylabel('Average Selling Price', fontsize=12)
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.title('Distribution of Selling Price', fontsize=16)
sns.histplot(df['Selling_Price'], kde=True, color='skyblue')
plt.xlabel('Selling Price')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.title('Spread of Selling Price', fontsize=16)
sns.boxplot(y=df['Selling_Price'], color='lightgreen')
plt.ylabel('Selling Price')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(25, 6))


plt.subplot(1, 3, 2)
fuel_counts = df['Fuel_Type'].value_counts()
plt.bar(fuel_counts.index, fuel_counts.values, color='lightcoral')
plt.title('Fuel Type Histogram', fontsize=16)
plt.xlabel('Fuel Type', fontsize=12)
plt.ylabel('Frequency of Fuel Type', fontsize=12)

plt.subplot(1, 3, 3)

transmission_counts = df['Transmission'].value_counts()
plt.bar(transmission_counts.index, transmission_counts.values, color='lightgreen')
plt.title('Transmission Type Histogram', fontsize=16)
plt.xlabel('Transmission Type', fontsize=12)
plt.ylabel('Frequency of Transmission Type', fontsize=12)

plt.tight_layout()
plt.show()

"""4-**Model Training Using XGBoost**

With the training data prepared, we proceed to train a regression model using the XGBoost algorithm, a powerful and efficient gradient boosting framework. XGBoost is particularly well-suited for structured tabular data and often outperforms traditional algorithms due to its ability to capture complex patterns through an ensemble of decision trees. During this step, the model learns to map the input features to the target selling price.
"""

X = df.drop('Selling_Price', axis=1)
y = df['Selling_Price']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_encoded = pd.get_dummies(X_train, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, drop_first=True)


X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)

model = XGBRegressor()
model.fit(X_train_encoded, y_train)

"""5-**Model Evaluation**

Once the model is trained, we evaluate its performance on the testing data using key regression metrics such as the R² score and Root Mean Squared Error (RMSE). The R² score indicates how well the model explains the variance in the target variable, while RMSE gives insight into the average prediction error. High R² values and low RMSE scores suggest that the model is both accurate and reliable.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


X = df.drop('Selling_Price', axis=1)
y = df['Selling_Price']


X_encoded = pd.get_dummies(X, drop_first=True)


X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)


model = XGBRegressor()
model.fit(X_train, y_train)


y_pred = model.predict(X_test)


rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")


plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred, color='teal', alpha=0.6)
plt.xlabel("Actual Selling Price")
plt.ylabel("Predicted Selling Price")
plt.title("Actual vs Predicted Selling Price")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')  # خط مثالي
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import joblib


joblib.dump(model, 'car_price_model.joblib')